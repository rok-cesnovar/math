<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.12"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Stan Math Library: stan::math::mpi_parallel_call&lt; call_id, ReduceF, CombineF &gt; Class Template Reference</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../$standoxy.css" rel="stylesheet" type="text/css">
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../stanlogo-main.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname"><a href="https://mc-stan.org/math">Stan Math Library</a>
   &#160;<span id="projectnumber">3.1.0</span>
   </div>
   <div id="projectbrief">Automatic Differentiation</div>
  </td>
   <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="../../search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.png" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.12 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html','../../');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-static-methods">Static Public Member Functions</a> &#124;
<a href="../../d8/d4f/classstan_1_1math_1_1mpi__parallel__call-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">stan::math::mpi_parallel_call&lt; call_id, ReduceF, CombineF &gt; Class Template Reference</div>  </div>
</div><!--header-->
<div class="contents">
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><h3>template&lt;int call_id, typename ReduceF, typename CombineF&gt;<br />
class stan::math::mpi_parallel_call&lt; call_id, ReduceF, CombineF &gt;</h3>

<p>The MPI parallel call class manages the distributed evaluation of a collection of tasks following the map - reduce - combine pattern. </p>
<p>The class organizes the distribution of all job related information from the root node to all worker nodes. The class discriminates between parameters and static data. The static data is only transmitted a single time and cached on each worker locally after the inital transfer.</p>
<p>The flow of commands are:</p>
<ol type="1">
<li>The constructor of this class must be called on the root node where all parameters and static data is passed to the class.</li>
<li>The constructor then tries to allocate the MPI cluster resource to obtain control over all workers in the cluster. If the cluster is locked already, then an exception is fired.</li>
<li>The worker nodes are instructed to run the static distributed_apply method of this class. This static method then instantiates a <a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html" title="The MPI parallel call class manages the distributed evaluation of a collection of tasks following the...">mpi_parallel_call</a> instance on the workers.</li>
<li>The root then broadcasts and scatters all necessary data to the cluster. Static data (including meta information on data shapes) are locally cached such that static data is only transferred on the first evaluation. Note that the work is equally distributed among the workers. That is N jobs are distributed ot a cluster of size W in N/W chunks (the remainder is allocated to node 1 onwards which ensures that the root node 0 has one job less).</li>
<li>Once the parameters and static data is distributed, the reduce operation is applied per defined job. Each job is allowed to return a different number of outputs such that the resulting data structure is a ragged array. The ragged array structure becomes known to <a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html" title="The MPI parallel call class manages the distributed evaluation of a collection of tasks following the...">mpi_parallel_call</a> during the first evaluation and must not change for future calls.</li>
<li>Finally the local results are gathered on the root node over MPI and given on the root node to the combine functor along with the ragged array data structure.</li>
</ol>
<p>The MPI cluster resource is acquired with construction of <a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html" title="The MPI parallel call class manages the distributed evaluation of a collection of tasks following the...">mpi_parallel_call</a> and is freed once the <a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html" title="The MPI parallel call class manages the distributed evaluation of a collection of tasks following the...">mpi_parallel_call</a> goes out of scope (that is, deconstructed).</p>
<p>Note 1: During MPI operation everything must run synchronous. That is, if a job fails on any of the workers then the execution must still continue on all other workers. In order to maintain a synchronized state, even the worker with a failed job must return a valid output chunk since the gather commands issued on the root to collect results would otherwise fail. Thus, if a job fails on any worker the a respective status flag will be transferred after the reduce and the results gather step.</p>
<p>Note 2: During the first evaluation of the function the ragged array sizes need to be collected on the root from all workers. This is needed on the root such that the root knows how many outputs are computed on each worker. This information is then cached for all subsequent evaluations. However, caching this information can occur if and only if the evaluation of all functions was successfull on all workers during the first run. Thus, the first evaluation is handled with special care to ensure that caching of this meta info is only done when all workers have successfully evaluated the function and otherwise an exception is raised.</p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">call_id</td><td>label for the static data </td></tr>
    <tr><td class="paramname">ReduceF</td><td>reduce function called for each job, </td></tr>
  </table>
  </dd>
</dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="../../d3/dd0/classstan_1_1math_1_1internal_1_1map__rect__reduce.html">internal::map_rect_reduce</a> </dd></dl>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">CombineF</td><td>combine function called on the combined results on each job along with the ragged data structure information, </td></tr>
  </table>
  </dd>
</dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="../../dd/d7f/classstan_1_1math_1_1internal_1_1map__rect__combine.html">internal::map_rect_combine</a> </dd></dl>

<p>Definition at line <a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html#l00161">161</a> of file <a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html">mpi_parallel_call.hpp</a>.</p>
</div>
<p><code>#include &lt;<a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html">mpi_parallel_call.hpp</a>&gt;</code></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a982f11599ebe02514d66f30cf932570a"><td class="memTemplParams" colspan="2">template&lt;typename T_shared_param , typename T_job_param &gt; </td></tr>
<tr class="memitem:a982f11599ebe02514d66f30cf932570a"><td class="memTemplItemLeft" align="right" valign="top">&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html#a982f11599ebe02514d66f30cf932570a">mpi_parallel_call</a> (const Eigen::Matrix&lt; T_shared_param, Eigen::Dynamic, 1 &gt; &amp;shared_params, const std::vector&lt; Eigen::Matrix&lt; T_job_param, Eigen::Dynamic, 1 &gt;&gt; &amp;job_params, const std::vector&lt; std::vector&lt; double &gt;&gt; &amp;x_r, const std::vector&lt; std::vector&lt; int &gt;&gt; &amp;x_i)</td></tr>
<tr class="memdesc:a982f11599ebe02514d66f30cf932570a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initiates a parallel MPI call on the root.  <a href="#a982f11599ebe02514d66f30cf932570a">More...</a><br /></td></tr>
<tr class="separator:a982f11599ebe02514d66f30cf932570a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adb656931b10dcb379014479dfe7ea2f4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html#adb656931b10dcb379014479dfe7ea2f4">mpi_parallel_call</a> ()</td></tr>
<tr class="separator:adb656931b10dcb379014479dfe7ea2f4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a60b27412905d30607b0695916acb7644"><td class="memItemLeft" align="right" valign="top">result_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html#a60b27412905d30607b0695916acb7644">reduce_combine</a> ()</td></tr>
<tr class="memdesc:a60b27412905d30607b0695916acb7644"><td class="mdescLeft">&#160;</td><td class="mdescRight">Once all data is distributed and cached the reduce_combine evaluates all assigned function evaluations locally, transfers all results back to the root and finally combines on the root all results.  <a href="#a60b27412905d30607b0695916acb7644">More...</a><br /></td></tr>
<tr class="separator:a60b27412905d30607b0695916acb7644"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-static-methods"></a>
Static Public Member Functions</h2></td></tr>
<tr class="memitem:a96db94f88d875ca947b83c2167de9c0a"><td class="memItemLeft" align="right" valign="top">static void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html#a96db94f88d875ca947b83c2167de9c0a">distributed_apply</a> ()</td></tr>
<tr class="memdesc:a96db94f88d875ca947b83c2167de9c0a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Entry point on the workers for the <a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html" title="The MPI parallel call class manages the distributed evaluation of a collection of tasks following the...">mpi_parallel_call</a>.  <a href="#a96db94f88d875ca947b83c2167de9c0a">More...</a><br /></td></tr>
<tr class="separator:a96db94f88d875ca947b83c2167de9c0a"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a982f11599ebe02514d66f30cf932570a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a982f11599ebe02514d66f30cf932570a">&sect;&nbsp;</a></span>mpi_parallel_call() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;int call_id, typename ReduceF, typename CombineF&gt; </div>
<div class="memtemplate">
template&lt;typename T_shared_param , typename T_job_param &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html">stan::math::mpi_parallel_call</a>&lt; call_id, ReduceF, CombineF &gt;::<a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html">mpi_parallel_call</a> </td>
          <td>(</td>
          <td class="paramtype">const Eigen::Matrix&lt; T_shared_param, Eigen::Dynamic, 1 &gt; &amp;&#160;</td>
          <td class="paramname"><em>shared_params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; Eigen::Matrix&lt; T_job_param, Eigen::Dynamic, 1 &gt;&gt; &amp;&#160;</td>
          <td class="paramname"><em>job_params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; std::vector&lt; double &gt;&gt; &amp;&#160;</td>
          <td class="paramname"><em>x_r</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; std::vector&lt; int &gt;&gt; &amp;&#160;</td>
          <td class="paramname"><em>x_i</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Initiates a parallel MPI call on the root. </p>
<p>The constructor allocates the MPI resource and initiates on all workers the MPI parallel call which mirror the communication and execute a chunk of the overall work.</p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">T_shared_param</td><td>type of shared parameters </td></tr>
    <tr><td class="paramname">T_job_param</td><td>type of job-specific parameters </td></tr>
  </table>
  </dd>
</dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">shared_params</td><td>shared parameter vector </td></tr>
    <tr><td class="paramname">job_params</td><td>array of job-specific parameter vectors </td></tr>
    <tr><td class="paramname">x_r</td><td>array of job-specific real arrays (data only argument) </td></tr>
    <tr><td class="paramname">x_i</td><td>array of job-specific int arrays (data only argument) </td></tr>
  </table>
  </dd>
</dl>

<p>Definition at line <a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html#l00204">204</a> of file <a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html">mpi_parallel_call.hpp</a>.</p>

</div>
</div>
<a id="adb656931b10dcb379014479dfe7ea2f4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adb656931b10dcb379014479dfe7ea2f4">&sect;&nbsp;</a></span>mpi_parallel_call() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;int call_id, typename ReduceF, typename CombineF&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html">stan::math::mpi_parallel_call</a>&lt; call_id, ReduceF, CombineF &gt;::<a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html">mpi_parallel_call</a> </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Definition at line <a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html#l00245">245</a> of file <a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html">mpi_parallel_call.hpp</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a96db94f88d875ca947b83c2167de9c0a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a96db94f88d875ca947b83c2167de9c0a">&sect;&nbsp;</a></span>distributed_apply()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;int call_id, typename ReduceF, typename CombineF&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">static void <a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html">stan::math::mpi_parallel_call</a>&lt; call_id, ReduceF, CombineF &gt;::distributed_apply </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Entry point on the workers for the <a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html" title="The MPI parallel call class manages the distributed evaluation of a collection of tasks following the...">mpi_parallel_call</a>. </p>

<p>Definition at line <a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html#l00256">256</a> of file <a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html">mpi_parallel_call.hpp</a>.</p>

</div>
</div>
<a id="a60b27412905d30607b0695916acb7644"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a60b27412905d30607b0695916acb7644">&sect;&nbsp;</a></span>reduce_combine()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;int call_id, typename ReduceF, typename CombineF&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">result_t <a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html">stan::math::mpi_parallel_call</a>&lt; call_id, ReduceF, CombineF &gt;::reduce_combine </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Once all data is distributed and cached the reduce_combine evaluates all assigned function evaluations locally, transfers all results back to the root and finally combines on the root all results. </p>

<p>Definition at line <a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html#l00269">269</a> of file <a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html">mpi_parallel_call.hpp</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>stan/math/prim/functor/<a class="el" href="../../d3/daf/mpi__parallel__call_8hpp_source.html">mpi_parallel_call.hpp</a></li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->

<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="../../da/dce/namespacestan.html">stan</a></li><li class="navelem"><a class="el" href="../../d4/d84/namespacestan_1_1math.html">math</a></li><li class="navelem"><a class="el" href="../../dc/d2e/classstan_1_1math_1_1mpi__parallel__call.html">mpi_parallel_call</a></li>
    <li class="footer">
    <div class="contents" style="font-size:100%;">
      <span style="float:left; margin=0 1em 0 1em;">
      &nbsp;&nbsp;&nbsp;&nbsp;
      [ <a href="http://mc-stan.org/">Stan Home Page</a> ]
      </span>
      <span style="float:right; margin=0 1em 0 1em;">
      <i>&copy; 2011&ndash;2019,
      Stan Development Team.
      &nbsp;&nbsp;&nbsp;&nbsp;
      </i>
      </span>
    </div> </li>
  </ul>
</div>
</body>
</html>
